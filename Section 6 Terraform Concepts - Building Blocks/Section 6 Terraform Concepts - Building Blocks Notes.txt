Content
=======


Section 6: Terraform Concepts - Building Blocks
25. Provision Software with Terraform
26. Lab : Provision Software with Terraform
27. DataSource in Terraform
28. Lab : DataSource in Terraform
29. Lab 2 : DataSource in Terraform
30. Output Attribute in TF
31. Lab : Output Attribute in TF
32. Remote State in Terraform
33. Lab : Remote State in Terraform



25. Provision Software with Terraform
=====================================

There are 2 ways to Provision Software on your Instance.

1st way - Build Custom AMI (Amazon Machine Image): - PREFFERABLE
--------------------------------------------------
	➢ Bundle your softwares and Files in Base Image.

	➢ Packer is Tool to Bundle the Custom AMIs.
		- Packer and Chef can help us create custom AIMs (with additional tools that we need for our specific stack)


2nd way - Boot Standard AMIs (Amazon Machine Image) and Install Software on Instance at Runtime.
------------------------------------------------------------------------------------------------
CONS:
	- when the base OS image is updated we can have discrapencies with the installation of the tools on it
	- installation on the runtime is not preffered

	➢ Using File Upload.
		- We can create scripts and upload them on the box
	➢ Using Remote-exec
		- We can then execute them with remote-exec
	➢ Using tools like Chef, Puppet & Ansible.



➢ Chef is Integrated with Terraform.

➢ User Can Run Puppet using Remote-Exec

➢ For Ansible, First Run Terraform, Get the Host IP address and then execute Ansible Playbook on Host.



File Upload on Instance
-----------------------

We have to provide, specify the file and its destination
------------------------------------------------------
resource "aws_instance" "MyFirstInstnace" {
  ami = lookup(var.AMIS, var.AWS_REGION)
  instance_type = "t2.micro"

  tags = {
    Name = "demoinstnce"
  }

  provisioner "file" {
    source = "installNginx.sh"
    destination = "/etc/installNginx.sh"
  }
}
------------------------------------------------------


➢ Remote-exec needs to be execute to execute the Script
	- we can use python script or shell script

➢ Terraform Provisioner needs to use SSH(Unix/Linux) or WinRM(Windows Machine)


User can use Connection to Make SSH Connection on Host.
-------------------------------------------------------

We must specify the user and password for the remote connection in order to execute the scripts
------------------------------------------------------
provisioner "file" {
  source = "installNginx.sh"
  destination = "/etc/installNginx.sh"

  connection {
    user = var.instance_user
    password = var.instance_pass
  }
}
------------------------------------------------------




On AWS User needs to use SSH KeyPairs instead of Password.
----------------------------------------------------------

If we use AZURE or GCP we can use SSH connection, but if we use AWS private/public key is the preffered way.

------------------------------------------------------
resource "aws_key_pair" "levelup-key" {			// define the connection resource type and name
  key_name = "levelup_key"				// define the private key
  public_key = "ssh rsa my-public-key"			// define the public key
}

resource "aws_instance" "MyFirstInstnace" {		// define the instance provider and name
  ami = lookup(var.AMIS, var.AWS_REGION)		// define the AMI
  instance_type = "t2.micro"				// tier of the machine
  key_name = aws_key_pair.levelup_key.key_name		// link the connection credentials

  tags = {
    Name = "custom_instance"				// tag thee instance machine
  }

  provisioner "file" {					// define the script resource file 
    source = "installNginx.sh"				// specify the name of the script file
    destination = "/etc/installNginx.sh"		// specify the location of the script file

    connection {					// define the connection
      user = var.instance_user				// link user 
      private_key = file(var.path_to_private_key)	// link the private key for the user
    }
  }
}
------------------------------------------------------

This is general template (may have some small variaties) to use private/public key combination to craete the connection to AWS machine



Remote-exec need to execute the Script.
---------------------------------------

This are the provisioner commands that will be executed
------------------------------------------------------
provisioner "remote-exec" {
  inline = [					// define list of commands taht will be executed
    "chmod +x /etc/installNginx.sh",		// set rights of the script file
    "/etc/installNginx.sh"			// execute the script file
  ]
}
------------------------------------------------------

2nd way summary
- Creating the isntance
- Installing the software 
	- prepare a script
	- create the connection on the box
	- upload and execute the script




26. Lab : Provision Software with Terraform
===========================================

We have exampel files for the lab
	➢ createInstace.tf
	➢ provider.tf
	➢ variables.tf
	➢ installNginx.sh


createInstace.tf
------------------------------------------------------
resource "aws_key_pair" "levelup_key" {		# declare resource aws key pair with name to connect the aws instance
    key_name = "levelup_key"				# define the name of the public key - we can use any name
    public_key = file(var.PATH_TO_PUBLIC_KEY)		# define the path to the public key variable
}

resource "aws_instance" "MyFirstInstnace" {		# define the instance type and name
  ami           = lookup(var.AMIS, var.AWS_REGION)	# link the AMI options
  instance_type = "t2.micro"				# set machine tier
  key_name      = aws_key_pair.levelup_key.key_name	# link the key from aws_key_pair resource

  tags = {						# instance tag
    Name = "custom_instance"
  }

  provisioner "file" {					# define provisioner - file
      source = "installNginx.sh"			# define the file name
      destination = "/tmp/installNginx.sh"		# define the file location path
  }

  provisioner "remote-exec" {				# define another prvisioner for the commands
    inline = [						# commands list
      "chmod +x /tmp/installNginx.sh",			# provide executable permissions the file
      "sudo sed -i -e 's/\r$//' /tmp/installNginx.sh",  # Remove the spurious CR characters if the file is created on nodepad++
      "sudo /tmp/installNginx.sh",			# execute the script file
    ]
  }

  connection {							# define connection
    host        = coalesce(self.public_ip, self.private_ip)	# receive private and public key
    type        = "ssh"						# specify the type of the connection
    user        = var.INSTANCE_USERNAME				# link the user
    private_key = file(var.PATH_TO_PRIVATE_KEY)			# link the private key
  }
}
------------------------------------------------------



provider.tf
------------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
------------------------------------------------------



variables.tf
------------------------------------------------------
variable "AWS_ACCESS_KEY" {				// define access key variable
    type = string
    default = "xxxxxxxxxxxxxxxx"
}

variable "AWS_SECRET_KEY" {}				// define empty secret access key

variable "AWS_REGION" {					// define regions variable with default value
default = "us-east-2"
}

variable "Security_Group"{				// define security group variable with default values
    type = list
    default = ["sg-24076", "sg-90890", "sg-456789"]
}

variable "AMIS" {					// set AIM IDs by region as default options
    type = map
    default = {
        us-east-1 = "ami-0f40c8f97004632f9"
        us-east-2 = "ami-05692172625678b4e"
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}

variable "PATH_TO_PRIVATE_KEY" {	// define variable for the AWS connection private key with default name value
  default = "levelup_key"
}

variable "PATH_TO_PUBLIC_KEY" {		// define variable for the AWS connection public key with default linked value
  default = "levelup_key.pub"		// if we have the keys in different directory, here we have to specify the full path
}

variable "INSTANCE_USERNAME" {		// define instance username with default value
  default = "ubuntu"
}
------------------------------------------------------


This is the file wew ill use to install nginx on the AWS instance

installNginx.sh
------------------------------------------------------
#!/bin/bash

# sleep until instance is ready
until [[ -f /var/lib/cloud/instance/boot-finished ]]; do
    sleep 1
done

# install nginx
apt-get update				// update package manager
apt-get -y install nginx		// install nginx

# make sure nginx is started
service nginx start

------------------------------------------------------



We must log into the work machine on DigitalOcean
	terminal --> ssh root@IP
	terminal --> yes/password


Create new directory and create the files or pull hte repo with the uploaded files
	terminal --> mkdir terrform_blocks

Navigate to the directory
	terminal --> cd terrform_blocks


Create all files with the actions below
---------------------------------------
- optional and more easy is to create a repository with the .tf files and pull the repo in the specific directory

Create the files
	terminal --> vi filename.tf

Insert the file content
	terminal --> insert		# to enter in insert mode
	terminal --> Shift + Insert	# to paste the file content with the correct indentation

Only the srcipt file should be in the root directory or the directory specified in the createInstace.tf/provisioner "file"/description


INIT
----
Initialize terraform
	terminal --> terraform init

We have to generate private and public keys
	terminal --> ssh-keygen -f levelup_key		# specify the file with the keys in it

	# We can se tsecurity phrase (optional)/enter for blank

List the files in the current direcotry
	termianl --> ls

	result: we must have levelup-key and levelup-key.pub files - the generated private and public keys

PLAN
----
Try to plan terraform resources
	terminal --> terraform plan
	terminal --> xxxxxxxxxxxxxxxxxxxxxxxxxxxxx 	# secret key

	# the plan should be successful
	# on the console we can see printed aws_key_pair public key
	

Before Applying the resources we need to modify the security group on AWS. We need to give access from our local PC.
- Login on AWS
- go to EC2/Secutiry Groups
- open the security group/inbound rules/Edit inbound rules/Add rule
	- Type: All traffic
	- Source: My IP			# this will apply the current IP that we use (local PC)
	- save rule

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm


We can see the logs on the console.

Check the resource creation on AWS/EC2. Because we added a rule for our local PC we can test the enginx on AWS. 


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> xxxxxxxxxxxxxxxxxxxxxx	# provide the seucrity key
	terminal --> yes 			# confirm




27. DataSource in Terraform
===========================

➢ Terraform provides DataSource for Certain Cloud Providers like AWS.

➢ DataSource Provides the dynamic Information about entities that are not managed by the current Terraform and configuration.

➢ AWS provides a lot of structured data which can be accessed via AWS APIs.

➢ Terraform expose this information using DataSources like AMIs, Availability Zones.

➢ Referencing a resource defined in a data source won’t create the resource itself, and your plan will fail if you reference nonexistent data or infrastructure.



➢ DataSource also provides the All IPs in use by AWS.

➢ This will help in IP Base Traffic filter.

➢ Traffic Filer on AWS can be done via Security Groups.

➢ Incoming and Outgoing Traffic can be done via Inbound Rules and Outbound Rules.

➢ Rules filter the traffic on the basis of Protocol, IP Range, and Ports.





28. Lab : DataSource in Terraform
=================================

We have 3 files
---------------
➢ createInstance.tf
➢ provider.tf
➢ variables.tf

We can find the correct AMI here - https://us-west-1.console.aws.amazon.com/ec2/home?region=us-west-1#AMICatalog:


createInstance.tf
------------------------------------------------------
data "aws_availability_zones" "available" {}				# fetch all AWS availabilit zones

data "aws_ami" "latest_ubuntu" {					# find AMI image
  most_recent = true
  owners      = ["099720109477"]					# official image owner

  filter {								# set filter to fetch images with names
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"] 
  }

  filter {								# set filter to fetch images with virtualization types
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

resource "aws_instance" "web" {
  ami           = data.aws_ami.latest_ubuntu.id				# data AMI image with the result of the filters
  instance_type = "t2.micro"						# instance tier
  availability_zone = data.aws_availability_zones.available.names[1]	# use the availability zone us-east-2b - first line 

  tags = {
    Name = "custom_instance"
  }
}
------------------------------------------------------


provider.tf
------------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
------------------------------------------------------


variables.tf
------------------------------------------------------
variable "AWS_ACCESS_KEY" {
    type = string
    default = "xxxxxxxxxxxxxxxxxxxx"
}

variable "AWS_SECRET_KEY" {}

variable "AWS_REGION" {
default = "us-east-2"
}

variable "Security_Group"{
    type = list
    default = ["sg-24076", "sg-90890", "sg-456789"]
}

variable "AMIS" {
    type = map
    default = {
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}
------------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Pull the repo
	terminal --> git clone repo_url

Navigate to the teraform working derectory
	terminal --> cd terraform

Install terraform if not installed
	terminal --> wget -O - https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(grep -oP '(?<=UBUNTU_CODENAME=).*' /etc/os-release || lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list
sudo apt update && sudo apt install terraform

If error appear execute the command
	terminal --> snap install terraform --classic

INIT
----
Initialize terrafomr
	terminal --> terraform init

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	# the plan should be successful and we can review the logs

With the plan we get the aim ID and availability zone




29. Lab 2 : DataSource in Terraform
===================================
In this lab we will create dynamically security group on AWS with terraform. We will restrict the traffic to specific ports on the machine.

We have 4 files
---------------
➢ createInstance.tf
➢ provider.tf
➢ variables.tf
➢ securitygroup.tf

We have one additional file from the last lab - securitygroup.tf

securitygroup.tf
------------------------------------------------------
data "aws_ip_ranges" "us_east_ip_range" {
    regions = ["us-east-1","us-east-2"]			# set AWS regions
    services = ["ec2"]					# set AWS service
}

resource "aws_security_group" "sg-custom_us_east" {
    name = "custom_us_east"

    ingress {						# set incoming traffic configuration
        from_port = "443"				# specify start port
        to_port = "443"					# specify end port 	1 port in total
        protocol = "tcp"				# specify the protocol
        cidr_blocks = slice(data.aws_ip_ranges.us_east_ip_range.cidr_blocks, 0, 50)
    }

    tags = {
        CreateDate = data.aws_ip_ranges.us_east_ip_range.create_date	# set create date of the security group
        SyncToken = data.aws_ip_ranges.us_east_ip_range.sync_token	# set the token for the security group
    }
}
------------------------------------------------------

Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Pull the repo
	terminal --> git clone repo_url

Navigate to the teraform working derectory
	terminal --> cd terraform_work_folder

INIT
----
Initialize terrafomr
	terminal --> terraform init

PLAN
----
Plan terraform resources
	terminal --> terraform plan -var AWS_REGION="us-east-1"
	# the plan should be successful and we can review the logs
	terminal --> AWS Secret access key

The plan should be successful - we can create 2 resources
	# aws_instance.web will be created						# Instance
     		 + ami                                  = "ami-0a7d80731ae1b2435"
    		 + arn                                  = (known after apply)
    		 + associate_public_ip_address          = (known after apply)
      		 + availability_zone                    = "us-east-1b"
	...
	# aws_security_group.sg-custom_us_east will be created				# security group
	+ resource "aws_security_group" "sg-custom_us_east" {
      	+ arn                    = (known after apply)
      	+ description            = "Managed by Terraform"
      	+ egress                 = (known after apply)
      	+ id                     = (known after apply)
      	+ ingress                = [
          + {
              + cidr_blocks      = [
                  + "100.24.0.0/13",
                  + "107.20.0.0/14",
                  + "110.238.2.0/23",
                  + "13.128.0.0/16",
	...
		

APPLY
-----
Apply the planned resources
	terminal --> terraform apply
Check on AWS/EC2 if the instance is created and AWS/EC2/Security group if the security group is created.
In the security group details we can see the avalable IPs for this instance.
	- on the tags page we can find the craetion date and SyncToken

DESTROY
-------
Destroy terraform resources
	terminal --> terrafomr destroy
	terminal --> xxxxxxxxxxxxxxxxxxxxxx	# provide the seucrity key
	terminal --> yes			# confirm destruction of the resources




30. Output Attribute in TF
==========================

DEPLOYMENT AUTOMATION
---------------------

➢ Terraform Keeps Output of all resources and it’s attribute.

➢ Output in terraform can be queried and retain.
	- retain the public/private IPs of the instaces in case of Ansible playbooks

➢ Output values are like the return values of a Terraform module, and have several uses:
	➢ A child module can use outputs to expose a subset of its resource attributes to a parent module
		- data from submodule can be accessed by the paret module
	➢ A root module can use outputs to print certain values in the CLI output after running terraform apply.
		- print out info for reporting purposes on aplly command
	➢ When using remote state, root module outputs can be accessed by other configurations via a terraform_remote_state data source.



➢ Each output value exported by a module must be declared using an output block:
------------------------------------------------------
output "instance_ip_addr" {				# define output block
  value = aws_instance.server.private_ip		# define instance parameter we want to get
}
------------------------------------------------------

➢ The value argument takes an expression whose result is to be returned to the user.

➢ Outputs are only rendered when Terraform applies your plan. Running terraform plan will not render outputs.
	- IMPORTANT - ONLY ON APPLY COMMAND, (NOT ON PLAN)



➢ Outputs can also be used in Scripts.

------------------------------------------------------
resource "aws_instance" "MyFirstInstnace" {			# instance
  ami = data.aws_ami.latest-ubuntu.id
  instance_type = "t2.micro"
  availability_zone = data.aws_availability_zones.avilable.names[1]

  tags = {
    Name = "custom_instance"
  }

  provisionor "local-exec" {								# local-exec provisioner
    command = "echo aws_instance.MyFirstInstnace.private_ip >> privateips.txt"		# save instance private IP in file
  }
}
------------------------------------------------------






31. Lab : Output Attribute in TF
================================

We have 4 files
---------------
➢ createInstance.tf
➢ provider.tf
➢ variables.tf
➢ securitygroup.tf

We will change only createInstance.tf file to set ouput block and print the instance's private IP on apply command

We can find all parameters of specific instance in providers documentation page - https://registry.terraform.io/browse/providers
	AWS - https://registry.terraform.io/providers/hashicorp/aws/latest/docs
	AZURE - https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs
	GCP - https://registry.terraform.io/providers/hashicorp/google/latest/docs
	... so on

For this example we will use AWS EC2 instance
	- go ro https://registry.terraform.io/providers/hashicorp/aws/latest/docs and search for 'ec2' in the search of the left
	- we can see all parameters


createInstance.tf
------------------------------------------------------

data "aws_availability_zones" "available" {}  # Fixed typo in "available"

data "aws_ami" "latest_ubuntu" {  # Changed to underscore (best practice)
  most_recent = true
  owners      = ["099720109477"]  # Canonical's official owner ID

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*"]  # Updated to current LTS
  }

  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

resource "aws_instance" "web" {
  ami           = data.aws_ami.latest_ubuntu.id
  instance_type = "t2.micro"
  availability_zone = data.aws_availability_zones.available.names[1]

  provisioner "local-exec" {					# define local-exec provisioner
    command = "echo ${self.private_ip} >> my_private_ips.txt"  	# save instance private ip in file
  }

  tags = {
    Name = "custom_instance"
  }
}

output "public_ip" {			# define output block
  value = aws_instance.web.public_ip	# print the instance private ip on apply command
}
------------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Pull the repo
	terminal --> git clone repo_url

Navigate to the teraform working derectory
	terminal --> cd terraform_work_folder

INIT
----
Initialize terrafomr
	terminal --> terraform init

PLAN
----
Plan terraform resources
	terminal --> terraform plan -var AWS_REGION="us-east-1"
	# the plan should be successful and we can review the logs
	terminal --> AWS Secret access key


result:
Plan: 2 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + public_ip = (known after apply)		# after the apply the public ip will be printed on the console
						# after the apply the file with the private IP also will be created

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

	We can check if the file is created and on the consol the public_ip is printed

Check the instance creation on AWS/EC2 and check the public and private IPs. Compare the private IP with the IP saved in the file locally.


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> xxxxxxxxxxxxxxxxxxxxxx	# provide the seucrity key
	terminal --> yes			# confirm destruction






32. Remote State in Terraform
=============================

➢ Terraform is able to find the resources it created and update them accordingly. We have seen Terraform Plan and terraform apply commands.

➢ Terraform records information about what infrastructure it created in a Terraform state file.

➢ File called terraform.tfstate

➢ Terraform also maintain the back-up of earlier statefile in file named terraform.tfstate.backup
	- When we have working terraform resources, and we make changes and apply them, a backup for the earlier state will be created

➢ On command Terraform Apply terraform backup is written and new state file created.




➢ If the remote state is changed and user executes terraform apply again. Terraform will make the changes to meet the correct remote state.

Problems while team is working on terraform
-------------------------------------------

➢ Shared storage for state files - To be able to use Terraform to update your infrastructure, each of your team members needs access to the same Terraform state files. That means you need to store those files in a shared location.

➢ Locking state files - As soon as data is shared, you run into a new problem: locking. Without locking, if two team members are running Terraform at the same time, you may run into race conditions as multiple Terraform processes make concurrent updates to the state files, leading to conflicts, data loss, and state file corruption.

➢ Isolating state files - When making changes to your infrastructure, it’s a best practice to isolate different environments.
	- separate direcories for the different environment managed by terraform

➢ Most common technique for allowing multiple team members to access a common set of files is to put them in version control (e.g. Git). But this is a bad idea.

➢ Manual error: It’s too easy to forget to pull down the latest changes from version control before running Terraform or to push
your latest changes to version control after running Terraform.

➢ Locking: Most version control systems do not provide any form of locking that would prevent two team members from running
terraform apply on the same state file at the same time.

➢ Secrets: All data in Terraform state files is stored in plain text. This is a problem because certain Terraform resources
need to store sensitive data.

➢ Instead of using version control, the best way to manage shared storage for state files is to use Terraform’s built-in support for remote backends. A Terraform backend determines how Terraform loads and stores state.



Remote Backend: Remote Backend solves all problems we listed earlier.
---------------------------------------------------------------------

➢ Manual error: Once you configure a remote backend, Terraform will automatically load the state file from that backend every time you run plan or apply and it’ll automatically store the state file in that backend after each apply, so there’s no chance of manual error

➢ Locking: Most of the remote backends natively support locking. To run terraform apply, Terraform will automatically acquire a lock; if someone else is already running apply, they will already have the lock, and you will have to wait.

➢ Secrets: Most of the remote backends natively support encryption in transit and encryption on disk of the state file.



Store state in S3 Bucket:
-------------------------

--------------------------------------------------
terraform {
  backend "s3" {
    bucket = "mybucket"
    key = "path/to/my/key"
    region = “us-east-1"
  }
}
--------------------------------------------------

➢ While using AWS S3 as backend, it’s recommended to use AWS configure instead of AWS creds in variables.
	- The backend service in terraform will NOT use the variables




33. Lab : Remote State in Terraform
===================================

We have to create S3 bucket in AWS
	- got to AWS/S3/Create bucket
		- Bucket type: Directory
		- Choose Zone: us-east-2		# or whatever is available
		- check 'I acknowledge that in the event of an Availability Zone outage, my data might be unavailable or lost.'
		- Bucket name: tf-state-4563rf		# must be globally unique name 
		- Encryption type: Server-side encryption with Amazon S3 managed keys (SSE-S3)
		- Create bucket


We have 5 files
---------------
4 from the last Lab
	➢ createInstance.tf
	➢ provider.tf
	➢ variables.tf
	➢ securitygroup.tf
1 new for terraform remote backend
	➢ backend.tf 


backend.tf 
--------------------------------------------------
terraform {
    backend "s3" {
        bucket = "tf-state-4563rf"				# the name of the created AWS bucket
        key    = "development/terraform_state"			# set the path of the terraform key
        region = "us-east-2"					# set the region of the created AWS S3 bucket region
    }
}
--------------------------------------------------



Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm

We need to configure AWS for the terraform backend
	terminal --> aws configure
	terminal --> AWS Access Key ID [None]: Access key
	terminal --> AWS Secret Access Key [None]: Secret Access key
	terminal --> Default region name [None]: us-west-2			# or whatever
	terminal --> Default output format [None]: enter

Navigate to the teraform working derectory
	terminal --> cd terraform_work_folder

INIT
----
We need to have created AWS S3 bucket.
Initialize terrafomr
	terminal --> terraform init

PLAN
----
Plan terraform resources
	terminal --> terraform plan -var AWS_REGION="us-east-1"
	# the plan should be successful and we can review the logs
	terminal --> AWS Secret access key

result:
Plan: 2 to add, 0 to change, 0 to destroy.



APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

On AWS/S3 we can enter the created S3 bucket and see that we have created 'development' directory. Inside we have file 'terraform_stete' in JSON format.
	➢ The Etag of the 'terraform_stete' file is the version of the current terraform state. We can see all Etags (versions) of the 'terraform_stete' file on this page.


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> xxxxxxxxxxxxxxxxxxxxxx	# provide the seucrity key
	terminal --> yes			# confirm destruction







