Content
=======

Section 9: Terraform Modules | Cloud Reusable
57. Terraform Module and Application
58. Lab : Terraform Source From GITHUB
59. Lab : Local Path Module
60. Lab : AWS VPC Module Part I
61. Lab : AWS VPC Module Part II
62. Lab : AWS VPC Module Part III




57. Terraform Module and Application
====================================

➢ How we can make the Infrastructure Code Reusable?


Problem with Terraform Config Structure
---------------------------------------

+--environments
| +--dev
| | +--main.tf
| |
| +--production
| | +--main.tf
| |
| +--staging
|   +--main.tf
|
+--main.tf
+--rovider.tf

➢ How User will add the New Resource Like Elastic Cache in above Structure?



Terraform Modules - Terraform Modules provides re-usable code.
-----------------

➢ With Terraform, user can put code inside of a Terraform module and reuse that module in multiple places.

➢ Modules are the key ingredient to writing reusable, maintainable, and testable Terraform code.

➢ Terraform’s way of creating modules is very simple: create a directory that holds a bunch of .tf files.

➢ Similar to functions in programming languages, module is reusable code that can be invoked multiple times with different inputs.

+--elasticache		# new directory
| +--main.tf		# new file and refer it to the different environments
|
+--environments
| +--dev
| | +--main.tf
| |
| +--production
| | +--main.tf
| |
| +--staging
|   +--main.tf
|
+--main.tf



➢ Below Syntax can be used to add elasticache in other Envs.

--------------------------------------------------
module "dev-elasticache" {				# module
  source = "../../elasticache"				# body of the module with relative or absolute path
}
--------------------------------------------------

We can use the module in each environment, but we must change the module name for the specific env.



Configurable Terraform Modules
------------------------------

➢ Now that user have our reusable module in place, user will hit another problem: each environment might have its own requirement from a certain resource.

➢ Eg. In dev we might need just one cache.m3.medium node in our Elasticache cluster, but in production, we might need 3 cache.m3.large nodes in the cluster.

➢ Above Issue can be solved by making the module configurable using Input varaibles.


+--elasticache
| +--main.tf
| +--variables.tf		# craete variables.tf for the module
|
+--environments
| +--dev
| | +--main.tf
| |
| +--production
| | +--main.tf
| |
| +--staging
|   +--main.tf
|
+--main.tf

➢ variables.tf file will hold the variables that configure the module.



➢ Sample Variable file.
--------------------------------------------------
variable "environment" {}
variable "node_count" {}
variable "node_type" {}
variable "availability_zones" { type = "list" }
--------------------------------------------------


Sample Module main.tf file.
--------------------------------------------------
resource "aws_elasticache_group" "elasticache-cluster" {		# define resource
  availability_zones = ["${var.availability_zones}"]			# set list of availability zones
  replication_group_id = "tf-${var.environment}-group"			# set replication groups
  replication_group_description = “${var.environment} group"		# rep gr description
  node_type = "${var.node_type}"					# node type
  number_cache_clusters = "${var.node_count}"				# coun tcache cluster
  parameter_group_name = “default.redis3.1"				# parameter group name - hard coded here
  port = 6379								# port
--------------------------------------------------



We use the variable template and configure the 'dev' environment

Module call in Dev Env
--------------------------------------------------
module "dev-elasticache" {				# define the module
  source = "../../elasticache"				# set template source
  environment = "dev"					# set env name
  node_count = 1					# node count
  node_type = “cache.m3.small"				# node hardware type
  availability_zones = ["us-east-1a", "us-east-1b"]	# list of availability zones
}
--------------------------------------------------


We use the same template but for different environment - production

Module call in prod Env
--------------------------------------------------
module "production-elasticache" {
  source = "../../elasticache"
  environment = “prod"
  node_count = 3
  node_type = "cache.m3.large"
  availability_zones = ["us-east-1a", "us-east-1b"]
}
--------------------------------------------------



Source of Modules
-----------------

➢ Registry
We can search for ready modules on terrafomr site - https://registry.terraform.io/
	- search for 'ec2 instances', click on 'see all' for modules
	# results are only for AWS (EC2 is aws resource)
	# https://registry.terraform.io/modules/terraform-aws-modules/ec2-instance/aws/latest#usage
	# we can see the module version, syntax and details

	In every module there is GitHub link
		- https://github.com/terraform-aws-modules/terraform-aws-ec2-instance

➢ GitHub
	- We can use terraform modules from GitHub bu cloning the code with GitHub CLI
		- https://github.com/terraform-aws-modules/terraform-aws-ec2-instance

➢ Local File Path
	- We refer local location of the create module






58. Lab : Terraform Source From GITHUB
======================================


We have 2 files
---------------
➢ main.tf	- basic configuration file
➢ proider.tf

To use created modules we need to provide only nessecary variables values. We can see which variables we must specify from the variables.tf in the repo. If no default values are set, we need to provide the variable and its value.
	- Module variables.tf - https://github.com/terraform-aws-modules/terraform-aws-ec2-instance/blob/master/variables.tf
	- We can see example usage - https://github.com/terraform-aws-modules/terraform-aws-ec2-instance?tab=readme-ov-file#usage

main.tf
--------------------------------------------------
module "ec2_cluster" {									# module name
    source = "github.com/terraform-aws-modules/terraform-aws-ec2-instance.git"		# we consume GitHub source
    # we can copy the module GitHub repo address and use it without 'https://' prefix

    # we can open variables.tf in the GitHub repo and see which variables do not have default values - they are required
    name    = "my-cluster"				# cluster name
    ami     = "ami-0f40c8f97004632f9"			# AMI (amazon machine image)
    instance_type          = "t2.micro"			# hardware type - t2.micro - free tier
    subnet_id   = "subnet-e92f9cc8"			# subnet name from AWS/VPC/Subnets in the us-east-1 region

    tags = {						# tags
    Terraform   = "true"
    Environment = "dev"
    }
}
--------------------------------------------------



proider.tf
--------------------------------------------------
provider "aws" {					# provider resource
  access_key = "AKIAY65Y5OPLU3XH5T6O"			# access key
  secret_key = ""					# secret key must be hidden, we will provide it in runtime
  region     = "us-east-1"				# region
}
--------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Generate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Delete the secret access key and access key from provider.tf
	terminal --> vi provider.tf
--------------------------------------------------
provider "aws" {
  region     = "us-east-1"
}
--------------------------------------------------
save changes - escape, :wq!, enter

Set Secret Access Key and Aceess Key as environemnt variable
	terminal --> export AWS_ACCESS_KEY="AKIAY65Y5OPLU3XH5T6O"
	terminal --> export AWS_SECRET_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

Check if the secret access key and access key are successfully set as environment variable
	terminal --> echo $AWS_SECRET_KEY
	terminal --> echo $AWS_ACCESS_KEY

Plan terraform resources
	terminal --> terraform plan

	# the plan should be successful and we can review the logs
	# result: 	
		Plan: 1 to add, 0 to change, 0 to destroy.

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> yes				# confirm

Check resource creation in AWS/EC2. This way we can use the module to create EC2 instance and provideonly required and managed variables.


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes






59. Lab : Local Path Module
===========================

We have 2 directories
--------------------
➢ custom_vpc
  ➢ Development
    ➢ Development
      - main.tf

➢ custom_vpc
  ➢ Development_QA
    ➢ Development_QA
      - main.tf


Development/Development/main.tf
--------------------------------------------------
module "dev-vpc"{
    source                          = "../../custom_vpc"

    vpcname                         = "dev01-vpc"
    cidr                            = "10.0.2.0/24"
    enable_dns_support              = "true"
    enable_classiclink              = "false"
    enable_classiclink_dns_support  = "false"
    enable_ipv6                     = "true"
    vpcenvironment                  = "Development-Engineering"

}
--------------------------------------------------


Development_QA/Development_QA/main.tf
--------------------------------------------------
module "dev-qa-vpc"{
    source                          = "../../custom_vpc"

    vpcname                         = "dev02-qa-vpc"
    cidr                            = "10.0.1.0/24"
    enable_dns_support              = "true"
    enable_classiclink              = "false"
    enable_classiclink_dns_support  = "false"
    enable_ipv6                     = "false"
    vpcenvironment                  = "Development-QA-Engineering"
    AWS_REGION                      = "us-east-1"
}
--------------------------------------------------

And 3 files
-----------
➢ custom_vpc
  ➢ vpc.tf
  ➢ variable.tf
  ➢ provider.tf


vpc.tf
-----------------------------------------------
# VPC
resource "aws_vpc" "aws_vpc_levelup" {

  cidr_block                       = var.cidr
  instance_tenancy                 = var.instance_tenancy
  enable_dns_hostnames             = var.enable_dns_hostnames
  enable_dns_support               = var.enable_dns_support
  # enable_classiclink               = var.enable_classiclink
  # enable_classiclink_dns_support   = var.enable_classiclink_dns_support
  assign_generated_ipv6_cidr_block = var.enable_ipv6

  tags = {
      name = var.vpcname
      environment = var.vpcenvironment
  }
}
-----------------------------------------------


variable.tf
-----------------------------------------------
variable "AWS_ACCESS_KEY" {
    type        = string
    default     = "AKIASMSIZOF42P2VUDSZ"
}

variable "AWS_REGION" {
    type        = string
    default     = "us-east-2"
}

variable "vpcname" {
  description = "Name to be used on all the resources as identifier"
  type        = string
  default     = ""
}

variable "cidr" {
  description = "The CIDR block for the VPC"
  type        = string
  default     = "0.0.0.0/0"
}

variable "instance_tenancy" {
  description = "A tenancy option for instances launched into the VPC"
  type        = string
  default     = "default"
}

variable "enable_dns_hostnames" {
  description = "Should be true to enable DNS hostnames in the VPC"
  type        = bool
  default     = false
}

variable "enable_dns_support" {
  description = "Should be true to enable DNS support in the VPC"
  type        = bool
  default     = true
}

variable "enable_classiclink" {
  description = "Should be true to enable ClassicLink for the VPC. Only valid in regions and accounts that support EC2 Classic."
  type        = bool
  default     = null
}

variable "enable_classiclink_dns_support" {
  description = "Should be true to enable ClassicLink DNS Support for the VPC. Only valid in regions and accounts that support EC2 Classic."
  type        = bool
  default     = null
}

variable "enable_ipv6" {
  description = "Requests an Amazon-provided IPv6 CIDR block."
  type        = bool
  default     = false
}

variable "vpcenvironment" {
  description = "AWS VPC Environment Name"
  type        = string
  default     = "Development"
}
-----------------------------------------------


provider.tf
-----------------------------------------------
provider "aws" {
  region     = var.AWS_REGION
}
-----------------------------------------------



Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Generate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Delete the secret access key and access key from provider.tf
	terminal --> vi provider.tf
--------------------------------------------------
provider "aws" {
  region     = "us-east-1"
}
--------------------------------------------------
save changes - escape, :wq!, enter

Set Secret Access Key and Aceess Key as environemnt variable
	terminal --> export AWS_ACCESS_KEY="AKIAY65Y5OPLU3XH5T6O"
	terminal --> export AWS_SECRET_KEY="xxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

Check if the secret access key and access key are successfully set as environment variable
	terminal --> echo $AWS_SECRET_KEY
	terminal --> echo $AWS_ACCESS_KEY

Plan terraform resources
	terminal --> terraform plan

	# the plan should be successful and we can review the logs
	# result: 	
		Plan: 1 to add, 0 to change, 0 to destroy.

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> yes				# confirm

Check resource creation in AWS/EC2. This way we can use the module to create EC2 instance and provideonly required and managed variables.


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes

➢ 



60. Lab : AWS VPC Module Part I
===============================

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 


61. Lab : AWS VPC Module Part II
================================

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 


62. Lab : AWS VPC Module Part III
=================================

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

➢ 

