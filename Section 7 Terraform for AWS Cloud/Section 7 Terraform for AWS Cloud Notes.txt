Content
=======

Section 7: Terraform for AWS Cloud
34. AWS VPC Introduction
35. AWS VPC Introduction II
36. Demo : AWS VPC & Security Group
37. Lab : Create AWS VPC & NAT Gateway
38. Launch EC2 Instance using Custom VPC
39. Lab : Launch EC2 Instance using Custom VPC
40. We Need You!!!
41. Elastic Block Store (EBS) in AWS
42. Demo : Elastic Block Store (EBS) in AWS
43. Lab : Elastic Block Store (EBS) in AWS
44. User Data in AWS
45. Lab: User Data using Script
46. Lab : User Data using Cloud Init




34. AWS VPC Introduction
========================

➢ Amazon Virtual Private Cloud (Amazon VPC) enables you to launch AWS resources into a virtual network that you've defined.

➢ Amazon VPC is the networking layer for Amazon EC2 Instances.

➢ For Fresh account Amazon have default VPC Network.
	- Amazon VPC is requirement to create EC2 instances

➢ Best Practice to always create your own VPC with your own Settings and Configuration.
	- always configure new VPC for each new EC2 instance


Few Key Points and Terminology
------------------------------

➢ Virtual private cloud (VPC) — A virtual network dedicated to your AWS account.

➢ Subnet — A range of IP addresses in your VPC.

➢ Route table — A set of rules, called routes, that are used to determine where network traffic is directed.

➢ Internet gateway — A gateway that you attach to your VPC to enable communication between resources in your VPC and the internet.

➢ VPC endpoint — Enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by
PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.



What user can do with VPC
-------------------------

➢ Launch instances in a subnet of your choosing. You can choose our own subnet addressing.

➢ You can assign custom IP address ranges in each subnet.
	- assign the resources that need access to internet into subnet that have access to internet

➢ You can configure route tables between subnets.

➢ You can create an internet gateway and attach it to our VPC.

➢ It provides much better security control over your AWS resources.

➢ You can assign security groups to individual instances.
	- what traffic the instance can accept and to whom the instance can access
	- can provide individual instance label

➢ You also have subnet network access control lists (ACLS).
	- define the control at subnet level inside of the VPC - that will make the VPC more easy to use

➢ For Small or Medium Setup One VPC will be enough.

➢ An Instance Launched in one VPC can never communicate to Instance Launched in another VPC via Private IP.
	- instance in one VPC can communicate only with another VPC true Private IP and not with instance in another VPC
	- instance in one VPC can communicate directly with instance in another VPC true Public IP

➢ Public IP is must to setup the inter VPC communication.

➢ Two VPCs can be linked via PVC Peering.



VPC Peering
-----------

➢ VPC Peering (bridge) is a networking connection that allows you to connect one VPC with another VPC through a direct network route using
private IP addresses.
	- this allow instances in different VPC to communicate true Private IP

➢ Instances behave as if they were on the same private network.

➢ You can peer VPC's with other AWS accounts as well as other VPCs in the same account.

➢ Peering is in a star configuration, i.e., 1 VPC peers other 4 VPCs.

➢ You can peer between regions. Suppose you have one VPC in one region and other VPC in another region, then you can peer the VPCs between different regions.

➢ One VPC can be connected via 1 or more VPCs.





35. AWS VPC Introduction II
===========================

Subnet in VPC
-------------

➢ Virtual private cloud (VPC) is a virtual network dedicated to your AWS account. It is logically isolated from other virtual
networks in the AWS Cloud.

➢ When you create a VPC, you must specify a range of IPv4 addresses for the VPC in the form of a Classless Inter-Domain Routing (CIDR) block.

➢ 10.0.0.0/16 is the primary CIDR block for your VPC.

➢ VPC spans all of the Availability Zones in the Region. After creating a VPC, you can add one or more subnets in each Availability Zone.
	- usually the regions have 3 or 4 avalability zones - a, b, c and/or d



Subnet in VPC
-------------

➢ If a subnet's traffic is routed to an internet gateway, the subnet is known as a public subnet.
	- the subnet have access to internet - public subnet

➢ If a subnet doesn't have a route to the internet gateway, the subnet is known as a private subnet.
	- the subnet DO NOT have access to internet - private subnet



VPC and subnet sizing for IPv4
------------------------------

We have 3 ranges:

➢ 10.0.0.0 - 10.255.255.255 (10/8 prefix) - User VPC must be /16 or smaller, for example, 10.0.0.0/16. - Default

➢ 172.16.0.0 - 172.31.255.255 (172.16/12 prefix) - User VPC must be /16 or smaller, for example, 172.31.0.0/16.

➢ 192.168.0.0 - 192.168.255.255 (192.168/16 prefix) - User VPC can be smaller, for example 192.168.0.0/20.



Subnet in VPC - To add a CIDR block to your VPC, the following rules apply:
---------------------------------------------------------------------------

➢ The allowed block size is between a /28 netmask and /16 netmask.

➢ CIDR block must not overlap with any existing CIDR block that's associated with the VPC.

➢ User cannot increase or decrease the size of an existing CIDR block.
	- We can add new CIDR block but not change the size of existing one


Security Group in AWS
---------------------

➢ Security group acts as a virtual firewall for your instance to control inbound and outbound traffic.

➢ Upto 5 SGs can be assigned to Instance in AWS.

➢ SGs are Instance Level not Subnet Level.
	- SG can be associated with instances and not networks


Basics of Security Group in AWS
--------------------------------

➢ User can specify allow rules, but not deny rules.
	- set incoming and outgoing rules
	- cannot define deny rule in the security group

➢ User can specify separate rules for inbound and outbound traffic.

➢ Security group rules enable you to filter traffic based on protocols and port numbers.
	- filter on port
	- filter on protocol

➢ By default, a Security Group don’t have any Inbound Rule.

➢ By default, a security group includes an outbound rule that allows all outbound traffic.

➢ There are quotas on the number of security groups that you can create per VPC, the number of rules that you can add to each security group, and the number of security groups that you can associate with a network interface.




36. Demo : AWS VPC & Security Group
===================================

How we can create a VPC:
➢ Go to AWS/VPC/Create VPC
	➢ Resources to create: VPC and more
	➢ Name tag auto-generation: levelup_custom_vpc
	➢ IPv4 CIDR block: 10.0.0.0/16			# Default
	➢ IPv6 CIDR block: No IPv6 CIDR block		# Default
	➢ Number of Availability Zones (AZs): 1 
	➢ Number of public subnets: 1			# Default
	➢ Number of private subnets: 1			# Default
	➢ NAT gateways ($): None			# Default
	➢ VPC endpoints: None
	➢ DNS options: checked [Enable DNS hostnames, Enable DNS resolution]
	➢ Create VPC

We can reviw our new VPC in AWS/VPC.

Now we can lounch instance and we can:		(we have to create the instance in the same region that the VPC was created)
--------------------------------------
	➢ use the newly created VPC in Network section
	➢ set a storage - default settings


We will create new security group
---------------------------------
➢ go to AWS/Security Groups/New Security Group
	➢ Security group name: levelup-sg
	➢ Description: Demo sg
	➢ VPC: select the newly created VPC
	➢ Create security group

Now we can lounch instance and we can:		(we have to create the instance in the same region that the VPC was created)
--------------------------------------
	➢ use the newly created VPC in Network section
	➢ set a storage - default settings
	➢ select the newly created security group




37. Lab : Create AWS VPC & NAT Gateway
======================================

We have 4 files
➢ provider.tf
➢ variables.tf
➢ vpc.tf
➢ nat.tf



provider.tf
--------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
--------------------------------------------------



variables.tf
--------------------------------------------------
variable "AWS_ACCESS_KEY" {
    type = string
    default = "AKIASMSIZOF42P2VUDSZ"
}

variable "AWS_SECRET_KEY" {}

variable "AWS_REGION" {
default = "us-east-2"
}

variable "Security_Group"{
    type = list
    default = ["sg-24076", "sg-90890", "sg-456789"]
}

variable "AMIS" {
    type = map
    default = {
        us-east-1 = "ami-0f40c8f97004632f9"
        us-east-2 = "ami-05692172625678b4e"
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}

variable "PATH_TO_PRIVATE_KEY" {
  default = "levelup_key"
}

variable "PATH_TO_PUBLIC_KEY" {
  default = "levelup_key.pub"
}

variable "INSTANCE_USERNAME" {
  default = "ubuntu"
}
--------------------------------------------------


VPC
=-=

We can see all parameters and syntax for the VPC - https://registry.terraform.io/providers/hashicorp/aws/latest/docs

AWS VPC
-------
➢ search 'aws vpc' in the search bar on left and choose 'aws_vpc'
   ➢ Example usage and syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc#example-usage
   ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/vpc#argument-reference

AWS SUBNET
----------
➢ search 'aws subnet'
   ➢ Example usage and syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet#example-usage
   ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/subnet#argument-reference

AWS INTERNET GATEWAY
--------------------
➢ search 'aws internet gateway'
  ➢ Example syntax - ttps://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/internet_gateway#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/internet_gateway#argument-reference

AWS ROUTE TABLE
---------------
➢ search 'aws route table'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table#argument-reference

AWS ROUTE TABLE ASSOCIATION
---------------------------
➢ search 'aws route table assocation'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association#argument-reference


vpc.tf
--------------------------------------------------
#Create AWS VPC
resource "aws_vpc" "levelupvpc" {
  cidr_block       = "10.0.0.0/16"			# Classless Inter-Domain Routing (CIDR) block.
  instance_tenancy = "default"		# more than one instance can run on the same hardware - preffered - minimized consts
  enable_dns_support   = "true"
  enable_dns_hostnames = "true"	  # create hostname and dns of our instance name, the name of our aws instance will work as dns

  tags = {
    Name = "levelupvpc"
  }
}

# Public Subnets in Custom VPC
resource "aws_subnet" "levelupvpc-public-1" {		# public subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.1.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "true"	# on launch the public IP will be asociated with this instance - public subnet
  availability_zone       = "us-east-2a"	# first availability zone for public subnet

  tags = {
    Name = "levelupvpc-public-1"		# name of the subnet
  }
}

resource "aws_subnet" "levelupvpc-public-2" {		# public subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.2.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "true"			# map the public IP to this ubnet - public subnet
  availability_zone       = "us-east-2b"		# second availability zone for public subnet

  tags = {
    Name = "levelupvpc-public-2"
  }
}

resource "aws_subnet" "levelupvpc-public-3" {		# public subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.3.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "true"			# map the public IP to this ubnet - public subnet
  availability_zone       = "us-east-2c"		# third availability zone for public subnet

  tags = {
    Name = "levelupvpc-public-3"
  }
}

# Private Subnets in Custom VPC				
resource "aws_subnet" "levelupvpc-private-1" {		# private subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.4.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "false"			# not mapping the public IP - private subnet
  availability_zone       = "us-east-2a"		# first availability zone for private subnet

  tags = {
    Name = "levelupvpc-private-1"
  }
}

resource "aws_subnet" "levelupvpc-private-2" {		# private subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.5.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "false"			# not mapping the public IP
  availability_zone       = "us-east-2b"		# second availability zone for private subnet

  tags = {
    Name = "levelupvpc-private-2"
  }
}

resource "aws_subnet" "levelupvpc-private-3" {		# private subnet
  vpc_id                  = aws_vpc.levelupvpc.id	# ID of our VPC
  cidr_block              = "10.0.6.0/24"		# IP range - we can't overlap with other subnets IP ranges
  map_public_ip_on_launch = "false"			# not mapping the public IP
  availability_zone       = "us-east-2c"		# second availability zone for private subnet

  tags = {
    Name = "levelupvpc-private-3"
  }
}

# Custom internet Gateway
resource "aws_internet_gateway" "levelup-gw" {		# internet Gateway resource and name
  vpc_id = aws_vpc.levelupvpc.id			# ID of our VPC

  tags = {
    Name = "levelup-gw"
  }
}

#Routing Table for the Custom VPC
resource "aws_route_table" "levelup-public" {		# routing table resource and name
  vpc_id = aws_vpc.levelupvpc.id			# ID of our VPC
  route {
    cidr_block = "0.0.0.0/0"				# all IPs
    gateway_id = aws_internet_gateway.levelup-gw.id	# our gateway ID
  }

  tags = {
    Name = "levelup-public-1"
  }
}

resource "aws_route_table_association" "levelup-public-1-a" {	# associate with our first public subnet
  subnet_id      = aws_subnet.levelupvpc-public-1.id		# our first public subnet id
  route_table_id = aws_route_table.levelup-public.id		# our route table id
}

resource "aws_route_table_association" "levelup-public-2-a" {	# associate with our second public subnet
  subnet_id      = aws_subnet.levelupvpc-public-2.id		# our second public subnet id
  route_table_id = aws_route_table.levelup-public.id		# our route table id
}

resource "aws_route_table_association" "levelup-public-3-a" {	# associate with our third public subnet
  subnet_id      = aws_subnet.levelupvpc-public-3.id		# our third public subnet id
  route_table_id = aws_route_table.levelup-public.id		# our route table id
}
--------------------------------------------------



With the lelp of the Internet Gateway and NAT Gateway our instance can communicate with internet.
We can find all NAT Gateway configurations in AWS documentation - 

EXTERNAL IP
-----------
➢ search 'aws eip'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eip#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eip#argument-reference


AWS NAT GATEWAY
---------------
➢ search 'aws_nat_gateway'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/nat_gateway#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/nat_gateway#argument-reference


AWS ROUTE TABLE
---------------
➢ search 'aws route table'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table#argument-reference


AWS ROUTE TABLE ASSOCIATION
---------------------------
➢ search 'aws route table assocation'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/route_table_association#argument-reference


nat.tf
--------------------------------------------------
#Define External IP 
resource "aws_eip" "levelup-nat" {			# external ip address
  domain   = "vpc"
}

resource "aws_nat_gateway" "levelup-nat-gw" {		# nat gateway
  allocation_id = aws_eip.levelup-nat.id		# external ip id
  subnet_id     = aws_subnet.levelupvpc-public-1.id	# our first public subnet id
  depends_on    = [aws_internet_gateway.levelup-gw]	# depends on our internet gateway
}

# VPC for the NAT Gateway so presented machine on internet CAN NOT access our machines on AWS
# Our AWS instance can access internet (egress allowed), external internet gateway connections not allowed (ingress is restricted)
resource "aws_route_table" "levelup-private" {		# aws route table 
  vpc_id = aws_vpc.levelupvpc.id			# our VPC id
  route {
    cidr_block     = "0.0.0.0/0"			# all IPs
    nat_gateway_id = aws_nat_gateway.levelup-nat-gw.id	# nat gateway id
  }

  tags = {
    Name = "levelup-private"
  }
}

# route associations private subnets with NAT Gateway
resource "aws_route_table_association" "level-private-1-a" {	# associate with our first private subnet
  subnet_id      = aws_subnet.levelupvpc-private-1.id		# our first private subnet id
  route_table_id = aws_route_table.levelup-private.id		# route table id
}

resource "aws_route_table_association" "level-private-1-b" {	# associate with our second private subnet
  subnet_id      = aws_subnet.levelupvpc-private-2.id		# our second private subnet id
  route_table_id = aws_route_table.levelup-private.id		# route table id
}

resource "aws_route_table_association" "level-private-1-c" {	# associate with our third private subnet
  subnet_id      = aws_subnet.levelupvpc-private-3.id		# our third private subnet id
  route_table_id = aws_route_table.levelup-private.id		# route table id
}	
--------------------------------------------------



Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	terminal --> AWS Secret access key

	# the plan should be successful and we can review the logs
	# result: 	Plan: 18 to add, 0 to change, 0 to destroy.

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

Go to AWS and set the correct region and check for the created resources.
Check on AWS/VPC that the VPC is created. 
Check on AWS/VPC/Subnets that all 6 subnets are created.
Check on AWS/VPC/Route Tables that we have private and public routing tables created
	- We can chcek the routes on AWS/VPC/Route Tables/Routes
	- We can check associated subnets on AWS/VPC/Route Tables/Subnets Associated
	- We can check the Tags on AWS/VPC/Route Tables/Tags
Check NAT Gatgeway on AWS/VPC/NAT Gateway
	- check Elastic IP
Check Internet Gateway on AWS/VPC/Internet Gateway





38. Launch EC2 Instance using Custom VPC
========================================

Requirements to create Instance with custom VPC
-----------------------------------------------

➢ Will see, how to launch the EC2 Instance using Custom VPC created via TF.

➢ We will be using Custom Security Group and KeyPair to login AWS machine.
	- We have to create security group 
	- We have to create KeyPair for login to AWS

➢ Security group works as a Firewall, managed by AWS.
	- Up to 5 security groups can be attached to single instance
	- define inbound rules
	- define outbound rules

➢ To SSH the machine, SSH inbound Rule must be defined in security group.
	➢ Allow ingress port 22 , IP address range 0.0.0.0/0 (All IPs)
		- we have to make sure that port 22 is not open for all users for all IP ranges
	➢ Best Practice to only all specified IPs.
		- we have to restrict access by IPs (specific group of PCs - office, home etc.)

➢ Create key pair and upload public key on AWS to SSH the AWS Instance.

➢ Private key always be need to present on your box not on AWS.





39. Lab : Launch EC2 Instance using Custom VPC
==============================================

We have 6 files:
----------------
➢ provider.tf
➢ variables.tf
➢ nat.tf
➢ vpc.tf
➢ security_group.tf
➢ createInstance.tf


First 4 files are from last Lab. New files are:
➢ security_group.tf
➢ createInstance.tf


AWS SECURITY GROUPS
-------------------
We can see all parameters and syntax for the VPC - https://registry.terraform.io/providers/hashicorp/aws/latest/docs
➢ search 'aws security group'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/security_group#argument-reference


security_group.tf
--------------------------------------------------
#Security Group for levelupvpc
resource "aws_security_group" "allow-levelup-ssh" {			# security group
  vpc_id      = aws_vpc.levelupvpc.id					# our VPC id
  name        = "allow-levelup-ssh"					# SG name
  description = "security group that allows ssh connection"		# short description

  egress {								# egress rule - outbond traffic
    from_port   = 0							# from port traffic accepted - all ports
    to_port     = 0							# to port traffic accepted - all ports
    protocol    = "-1"							# "-1" - all protocols, we can use also list []
    cidr_blocks = ["0.0.0.0/0"]						# all IPs for outgoing traffic - best practice
  }

  ingress {								# ingress rule - inbound traffic			
    from_port   = 22							# from port 22 only incoming traffic
    to_port     = 22							# to port 22 only incoming traffic
    protocol    = "tcp"							# tcp protocol only
    cidr_blocks = ["0.0.0.0/0"]	 					# all IPs - not recommended
  } Pest prectice - We should specify only the IP of our local PC/box that will execute terraform commands
  
  tags = {								# tag
    Name = "allow-levelup-ssh"
  }
}
--------------------------------------------------


createInstance.tf
--------------------------------------------------

resource "aws_key_pair" "levelup_key" {			# create KeyPair
    key_name = "levelup_key"				# key name
    public_key = file(var.PATH_TO_PUBLIC_KEY)		# key path to location
}

resource "aws_instance" "MyFirstInstnace" {		# instance
  ami           = lookup(var.AMIS, var.AWS_REGION)	# image
  instance_type = "t2.micro"				# tier hardware
  key_name      = aws_key_pair.levelup_key.key_name	# key name

  vpc_security_group_ids = [aws_security_group.allow-levelup-ssh.id]	# security group ids
  subnet_id = aws_subnet.levelupvpc-public-2.id			# 2nd public subnet id, so aws do not take the default subnet

  tags = {								# tag
    Name = "custom_instance"
  }

}
--------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Creatate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	terminal --> AWS Secret access key

	# the plan should be successful and we can review the logs
	# result: 	Plan: 21 to add, 0 to change, 0 to destroy.

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

Login to AWS and go set the correct region configured - us-east-2
Check resources creation on AWS/EC2:
Check security group on AWS/EC2/Security Group
Check inbound and outbound rules on AWS/EC2/Security Group/Security 


We can now try to connect the AWS instance from local PC/box.
Copy the public IP from AWS/EC2.
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

	# ssh instance_public_ip		- connect to the AWS instance
	# -l ubuntu				- used user
	# -i levelup_key			- private key

	terminal --> yes		# when ask for connection fingerprint

Now we are logged on the AWS instance
	Use root user
		terminal --> sudo -s

	Update package manager
		terminal --> suco apt-get update

		# this mean we have connection to the internet - egress connection

	Install wget and curl
		terminal --> apt-get install wget
		terminal --> apt-get install curl

	Open google tab
		terminal --> curl https://google.com

		# result: we get the html of google homepage

	Colse the connection of AWS isntance
		terminal --> exit



DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes			# confirm destruction





40. We Need You!!!
==================

Hello Cloud Geeks

Sorry to break you up here. I need your feedback.

Future students of this course would love to see your opinions! Please take 10-15 seconds to leave a review of this course.


You can modify your feedback anytime during learning, after completing, or even after your Certification Exam.

5 Star Rating Appreciated :)! Even if you have suggestions you are always welcome!


Thank You in advance!!!!

Happy Learning!





41. Elastic Block Store (EBS) in AWS
====================================

➢ EBS - Elastic Block Storage. EBS volume is a durable, block-level storage device that you can attach to your instances.

➢ EBS is like a secondary disk with Instance, which is flexible. Two types:
	- boot disk - when the instance is rebooted or destroyed, the boot disk is also destroyed
	- secondary disk (EBS) - permanent disk, not erased when instance is rebboted or destroyed

➢ User can dynamically increase size, modify the provisioned IOPS capacity, and change volume type on live production volumes.
	- we can manage the syze of the disk without stop or terminate the instace 
	- we can modify I/O capacity
	- we can change Disk type 
	- same disk will be attached when instance is updated (rebooted, recreated, updated)

➢ User can attach multiple EBS volumes to a single instance.
	- we can attach different types of disks on one instance
		- different type of data are configured to be saved on specific disk partitions		

➢ EBS volume and instance must be in the same Availability Zone.



Types of AWS EBS Volumes - Two Amazon EBS volume type categories: SSD-backed volumes and HDD-backed volumes.
------------------------

➢ SSD-backed volumes are optimized for transactional workloads, where the volume performs a lot of small read/write operations. The performance of such volumes is measured in IOPS (input/output operations per second)

➢ HDD-backed volumes are designed for large sequential workloads where throughput is much more important (and the performance is measured with MiB/s).

-----------------------------------------------------------------
Solid State Drive (SSD)		|	Hard Disk Drive (HDD)	|
--------------------------------|--------------------------------
General Purpose SSD		|Throughput Optimized HDD	|
Balanced for economy and	|Inexpensive, for high use,	|
performance			|intensive workload		|
--------------------------------|-------------------------------|
Provisioned IOPS SSD		|Cold HDD			|
High Performance, for important |Cheap, used for infreaquent	|
applications			|access				|
				|				|
				|				|
-----------------------------------------------------------------


General Purpose SSD (gp2) - Balanced for price and performance and recommended for most use cases.
-------------------------
	➢ Max 3000 IOPS allowed in gp2.
	➢ Sizes of gp2 volumes can vary from 1 GiB to 16 TiB, while maximum throughput is capped at 160 MiB/s.


Provisioned IOPS SSD (io1) - Used for critical production applications and databases that need the high performance (up to 32,000
--------------------------    IOPS).
	➢ Io1 volume sizes vary from 4 GiB to 16 TiB, while throughput is maxed at 500 MiB/s


Throughput Optimized HDD (st1) - HDD is designed for applications that require larger storage and bigger throughput, such as big 
------------------------------	 data or data warehousing, where IOPS is not that relevant.
	➢ Maximum throughput is capped at 500 MiB/s, sizes vary from starting 500 GiB to 16 TiB.
	
	
Cold HDD (sc1) - Cold HDD (sc1) is a magnetic storage format suitable for scenarios where storing data at low cost is the main
--------------   criteria. Sizes vary from 500 GiB to 16 TiB, throughput can reach 250 MiB/s.




42. Demo : Elastic Block Store (EBS) in AWS
===========================================

We will launch new instance on AWS:
	Go to AWS/EC2/Launch new instance
		➢ Application and OS Images (Amazon Machine Image)
			- Amazon Linux / 64 bit
			- Amazon Machine Image (AMI): Amazon Machine 2023 karnel-6.1 AMI 	# free tier
		➢ Instance type: t2.micro							# free tier
		
		➢ Network settings
			- Network: vpc
			- Subnet No-preference (default subnet in any avalability zone)
			- Auto-assign public IP: Enabled
			- Firewall (security groups): Create security Group
				- cheched 'Allow SSH traffic from' - My IP

		➢ Configure storage
			- 8 GB, gp3 - Root volume, 3000 IOPS, Not encrypted	# this is the default Disk (boot disk)
			- We can add new one and see the types

		➢ Launch the instance

To manage storages we have to wait intil the instance is in running state.

Whe nthe isntance is in running state go to Elastik Block Storage (EBS) / Volumes
		➢ We can see details for each volume we have
	Create new volume:
		➢ Volume settings
			- Volume type: Cnhoose a suitable type
			- Size: Choose appropriate size
			- IOPS: --
			- Throughput (MiB/s)
			- Availability Zone: xxxxa, xxxxb, xxxxc		# must be the same as the instance
			- Snapshot ID - optional: 
			- Encryption: check - optional

		➢ Tags - optional
		➢ Create Volume

We can now attach the newly crated volume to existing instance.

Terminate the instance on AWS/EC2.
We can see that the craeted volume is still available after the instance is terminated.




43. Lab : Elastic Block Store (EBS) in AWS
==========================================

We have 3 files
➢ createInstance.tf
➢ provder.tf
➢ variables.tf


provder.tf
--------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
--------------------------------------------------


AWS EBS VOLUME
--------------
➢ search 'aws_ebs_volume'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ebs_volume#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ebs_volume#argument-reference

AWS VOLUME ATTACHEMENT
----------------------
➢ search 'aws_volume_attachment'
  ➢ Example syntax - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/volume_attachment#example-usage
  ➢ Arguments references - https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/volume_attachment#argument-reference


createInstance.tf
--------------------------------------------------

resource "aws_key_pair" "levelup_key" {             	# KeyPair for login
    key_name = "levelup_key"				# public key name
    public_key = file(var.PATH_TO_PUBLIC_KEY)		# public key location path
}

#Create AWS Instance
resource "aws_instance" "MyFirstInstnace" {		# AWS instance
  ami           = lookup(var.AMIS, var.AWS_REGION)	# amazon machine image AMI
  instance_type = "t2.micro"				# type hardware - t2.micro is free tier
  availability_zone = "us-east-2a"			# zone
  key_name      = aws_key_pair.levelup_key.key_name	# access key name

  tags = {						# tag
    Name = "custom_instance"
  }
}

#EBS resource Creation
resource "aws_ebs_volume" "ebs-volume-1" {		# Volume
  availability_zone = "us-east-2a"			# region - must be same as the instance region
  size              = 50				# size
  type              = "gp2"				# type

  tags = {						# tag
    Name = "Secondary Volume Disk"
  }
}

#Atatch EBS volume with AWS Instance				
resource "aws_volume_attachment" "ebs-volume-1-attachment" {	# volume attachement
  device_name = "/dev/xvdh"		#  (Required) The device name to expose to the instance (for example, /dev/sdh or xvdh)
  volume_id   = aws_ebs_volume.ebs-volume-1.id			# our ebs volume id
  instance_id = aws_instance.MyFirstInstnace.id			# our instance id
}
--------------------------------------------------



variables.tf
--------------------------------------------------
variable "AWS_ACCESS_KEY" {
    type = string
    default = "AKIAY65Y5OPLU3XH5T6O"
}

variable "AWS_SECRET_KEY" {}

variable "AWS_REGION" {
default = "us-east-2"
}

variable "AMIS" {
    type = map
    default = {
        us-east-1 = "ami-0f40c8f97004632f9"
        us-east-2 = "ami-05692172625678b4e"
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}

variable "PATH_TO_PRIVATE_KEY" {
  default = "levelup_key"
}

variable "PATH_TO_PUBLIC_KEY" {
  default = "levelup_key.pub"
}

variable "INSTANCE_USERNAME" {
  default = "ubuntu"
}
--------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Generate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	terminal --> AWS Secret access key

	# the plan should be successful and we can review the logs
	# result: 	Plan: 4 to add, 0 to change, 0 to destroy.	

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

Login to AWS and go set the correct region configured - us-east-2
Check resources creation on AWS/EC2:
Check if volumes are created on AWS/EC2/Volumes


We can now try to connect the AWS instance from local PC/box.
Copy the public IP from AWS/EC2.
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

	# ssh instance_public_ip		- connect to the AWS instance
	# -l ubuntu				- used user
	# -i levelup_key			- private key

	terminal --> yes		# when ask for connection fingerprint

Now we are loged on aour AWS instance
	switch to the root user
	instance terminal --> sudo -s

List volumes to check all disks
	instance terminal --> df -h		

	# we can see only the boot/default volume
	# the attached disk is not listed

To use the attached volume we need to clear the file system on our disk and alocate the attached disk

Clear instance file system
	instance terminal --> mkfs.ext4 /dev/xvdh		# '/dev/xvdh' - the type of the attached volume

Create directory and mount the attached  disk
	instance terminal --> mkdir -p /data			# create data directory
	instance terminal --> mount /dev/hvdh /data		# mount the colume with the local directory


Now we can work with the disk but when we reboot the machine, the attached disk will be destroyed. How to avoid that?

Open fstab config file
	instance terminal --> vim fstab		

the first line is existing. We add the second line '/dev/xvdh /data ext4 defaults 0 0'
-------------------------------------------------
LABEL=cloudimg-rootfs	/ 	ext4	defaults,discard	0 0
/dev/xvdh /data ext4 defaults 0 0
-------------------------------------------------
save chanes - escpace, :wq!, enter

Now e can reboot our instance on AWS/EC2/instance/actions/reboot
Copy the public IP from AWS/EC2.
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

List volumes to check all disks
	instance terminal --> df -h

	# the attached volume is present


Now the volume will not dissapear but the data in the volume will dissapear. How to solve?

Create a file in the volume to check data availability
	instance terminal --> echo 'Hi, this is Jordan' > test.txt

We will check when we terminate the instance if the data and the volume will be deleted.
Terminate the instance from AWS/EC2 dashboard

Use terraform to plan and create the instance again	
	terminal --> terraform plan			
	
	# we can see what is goint to be created - 2 resources only
		- aws_instance.MyFirstInstance				# the instance
		- aws_volume_attachment.ebs-volume-1-attachment		# volume attahcment

	Apply the planned resources
	terminal --> terraform apply
	terminal --> yes		# confirm 
		
	
Login to the instance again 
Copy the public IP from AWS/EC2.
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

	# ssh instance_public_ip		- connect to the AWS instance
	# -l ubuntu				- used user
	# -i levelup_key			- private key

	terminal --> yes		# when ask for connection fingerprint

Now we are loged on aour AWS instance
	switch to the root user
	instance terminal --> sudo -s

List volumes to check all disks
	instance terminal --> df -h

	# the disk is not presented


Create directory and mount the attached  disk
	instance terminal --> mkdir -p /data			# create data directory
	instance terminal --> mount /dev/hvdh /data		# mount the colume with the local directory

Navigate to /data folder and list files
	instance terminal --> cd /data
	instance terminal --> ls

	# we can se that the test file is present	


DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes			# confirm destruction







44. User Data in AWS
====================

How we can pass User Data in time of instance launch
----------------------------------------------------

➢ In AWS, user have option to pass user data to the instance at launch.
	- configuration script
	- install script
	- execute script

➢ User data can be used to perform common automated configuration tasks and even run scripts after the instance starts.

➢ User can pass two types of user data to Amazon EC2: shell scripts and cloud-init directives.
	- simple shell script - will execute after the instance is lounched
	- cloud-init directive - will execute after the instance is lounched

➢ User data shell scripts must start with the #! characters and the path to the interpreter you want to read the script (commonly /bin/bash).
	- first line of the script must be '#! /bin/bash' - configure the interpreter that will execute the script

➢ Scripts entered as user data are executed as the root user, so do not use the sudo command in the script.
	- no 'sudo' in teh script because the script is executed by the root user

➢ By default, user data scripts and cloud-init directives run only during the first boot cycle when an EC2 instance is launched.
	- the sripts will be exeuted only once at the instance lounch
	- the sript will NOT execute on stop/start or reboot of the instance

➢ If user stop an instance, modify the user data, and start the instance, the new user data is not executed automatically.
	- modifying user data and stop/start ro reboot the instance will NOT execute the script automatically




45. Lab: User Data using Script
===============================

We have 3 files
➢ createInstance.tf
➢ provider.tf
➢ variables.tf
➢ installapache.sh


createInstance.tf
--------------------------------------------------
resource "aws_key_pair" "levelup_key" {			# KeyPair for terraform AWS login
    key_name = "levelup_key"				# key name
    public_key = file(var.PATH_TO_PUBLIC_KEY)		# public key path to location
}

#Create AWS Instance
resource "aws_instance" "MyFirstInstnace" {		# aws instance
  ami           = lookup(var.AMIS, var.AWS_REGION)	# amazone machine image AMI - image and region
  instance_type = "t2.micro"				# type hardware - t2.micro - free tier
  availability_zone = "us-east-2a"			# zone - a, b, c or d
  key_name      = aws_key_pair.levelup_key.key_name	# path of the access key

  user_data = file("installapache.sh")			# path to user data file - script

  tags = {						# tag
    Name = "custom_instance"
  }
}

output "public_ip" {					# print public IP of the instance on the console on launch
  value = aws_instance.MyFirstInstnace.public_ip 
}

--------------------------------------------------


provider.tf
--------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
--------------------------------------------------


variables.tf
--------------------------------------------------
variable "AWS_ACCESS_KEY" {
    type = string
    default = "AKIAY65Y5OPLU3XH5T6O"
}

variable "AWS_SECRET_KEY" {}

variable "AWS_REGION" {
default = "us-east-2"
}

variable "AMIS" {
    type = map
    default = {
        us-east-1 = "ami-0f40c8f97004632f9"
        us-east-2 = "ami-05692172625678b4e"
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}

variable "PATH_TO_PRIVATE_KEY" {
  default = "levelup_key"
}

variable "PATH_TO_PUBLIC_KEY" {
  default = "levelup_key.pub"
}

variable "INSTANCE_USERNAME" {
  default = "ubuntu"
}
--------------------------------------------------



installapache.sh
--------------------------------------------------
#! /bin/bash						# configure interpreter with root user
apt-get update						# update package manager
apt-get install -y apache2				# install apache2
systemctl start apache2									# start apache2
systemctl enable apache2								# activate apache2
echo "<h1>Deployed Machine via Terraform</h1>" | sudo tee /var/www/html/index.html	# print message

--------------------------------------------------

Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Generate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	terminal --> AWS Secret access key

	# the plan should be successful and we can review the logs
	# result: 	
		Plan: 2 to add, 0 to change, 0 to destroy.

		Changes to Outputs:
  		+ public_ip = (known after apply)	

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

On the console we can see instance public ip 
Login to AWS and go set the correct region configured - us-east-2
Check resources creation on AWS/EC2:

We can connect the AWS instance from local PC/box.
Copy the public IP from the console
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

	# ssh instance_public_ip		- connect to the instance
	# -l ubuntu				- login as ubuntu user
	# -i levelup_key			- use terraform public key

Now we are logged into the instance.

Check the status of the installed apache2
	instance terminal --> systemctl status apache2

We can browse the public IP of the AWS instance and we should receive the message from the script 'Deployed Machine via Terraform' 

If we cannot connect to the instance we have to go to AWS/EC2/Security Group/Inbound Traffic and allow our local PC IP

Exit the AWS instance
	instance terminal --> exit

DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes			# confirm destruction







46. Lab : User Data using Cloud Init
====================================

We have 5 files
➢ createInstance.tf
➢ provider.tf
➢ variables.tf
➢ init.cfg
➢ cloudinit.tf


init.cfg
--------------------------------------------------
#cloud-config

repo_update: true		
repo_upgrade: all		# this will update all packages on our linux box

packages:			# define packages for install
- apache2

runcmd:					# set execution of commands
- systemctl start apache2		# start apache service
- sudo systemctl enable apache2		# enable apache
- [ sh, -c, 'echo "<h1>Deployed Machine via Terraform</h1>" | sudo tee /var/www/html/index.html']	# print success message

output:
all: '| tee -a /var/log/cloud-init-output.log'		# save log into file cloud-init-output.log
--------------------------------------------------



cloudinit.tf
--------------------------------------------------
data "template_file" "install-apache" {			# set template file
    template = file("init.cfg")				# path to location to config file
}

data "template_cloudinit_config" "install-apache-config" {	# define template file
    gzip          = false					
    base64_encode = false					# not encoded

    part {						
        filename     = "init.cfg"
        content_type = "text/cloud-config"
        content      = data.template_file.install-apache.rendered
    }
}
--------------------------------------------------


createInstance.tf
--------------------------------------------------
resource "aws_key_pair" "levelup_key" {			# KeyPair for terraform to connect AWS
    key_name = "levelup_key"				# key name
    public_key = file(var.PATH_TO_PUBLIC_KEY)		# key path to location
}

#Create AWS Instance
resource "aws_instance" "MyFirstInstnace" {		# instance
  ami           = lookup(var.AMIS, var.AWS_REGION)	# amazone machine image - image AMI, region
  instance_type = "t2.micro"				# type machine - t2.micro - free tier
  availability_zone = "us-east-2a"			# zone
  key_name      = aws_key_pair.levelup_key.key_name	# key name

  user_data = data.template_cloudinit_config.install-apache-config.rendered	# path to template file - script

  tags = {						# tag
    Name = "custom_instance"
  }
}

output "public_ip" {					# print instance public ip on launch
  value = aws_instance.MyFirstInstnace.public_ip 
}
--------------------------------------------------


provider.tf
--------------------------------------------------
provider "aws" {
  access_key = var.AWS_ACCESS_KEY
  secret_key = var.AWS_SECRET_KEY
  region     = var.AWS_REGION
}
--------------------------------------------------


variables.tf
--------------------------------------------------
variable "AWS_ACCESS_KEY" {
    type = string
    default = "AKIAY65Y5OPLU3XH5T6O"
}

variable "AWS_SECRET_KEY" {}

variable "AWS_REGION" {
default = "us-east-2"
}

variable "AMIS" {
    type = map
    default = {
        us-east-1 = "ami-0f40c8f97004632f9"
        us-east-2 = "ami-05692172625678b4e"
        us-west-2 = "ami-0352d5a37fb4f603f"
        us-west-1 = "ami-0f40c8f97004632f9"
    }
}

variable "PATH_TO_PRIVATE_KEY" {
  default = "levelup_key"
}

variable "PATH_TO_PUBLIC_KEY" {
  default = "levelup_key.pub"
}

variable "INSTANCE_USERNAME" {
  default = "ubuntu"
}
--------------------------------------------------


Login to the igitalOcean ubuntu and pull the files from github.
	terminal --> ssh root@IP
	terminal --> password

Update the linux package manager
	terminal --> sudo apt-get update

Pull the repo
	terminal --> git clone repo_url
	or
	terminal --> git pull

We need to install AWS CLI on the machine
	terminal --> sudo apt-get install awscli
	terminal --> y					# confirm


INIT
----
Initialize terrafomr
	terminal --> terraform init

Generate private and public key
	terminal --> ssh-keygen -f levelup_key
	terminal --> enter
	terminal --> enter

Verify key creation
	terminal --> ls
	# we should have 2 new files - levelup-key and levelup-key.pub

PLAN
----
Plan terraform resources
	terminal --> terraform plan
	terminal --> AWS Secret access key

	# the plan should be successful and we can review the logs
	# result: 	
		Plan: 2 to add, 0 to change, 0 to destroy.

		Changes to Outputs:
  		+ public_ip = (known after apply)	

APPLY
-----
Apply the plan made on the DigitalOcean Ubuntu machine
	terminal --> terraform apply
	terminal --> xxxxxxxxxxxxxxxxxxxxxx		# provide the seucrity key
	terminal --> yes				# confirm

On the console we can see instance public ip. Copy it and use it to login to the instance.

We can connect the AWS instance from local PC/box.
Copy the public IP from the console
	terminal --> ssh instance_public_ip -l ubuntu -i levelup_key

	# ssh instance_public_ip		- connect to the instance
	# -l ubuntu				- login as ubuntu user
	# -i levelup_key			- use terraform public key

Now we are logged into the instance.

List files in the base directory
	instance terminal --> ls

	# cloud-init.log file should be available

Print the log file to see the logs
	instance terminal --> cat cloud-init.log

Chck for apache2 status
	instance terminal --> systemctl apache2
	
	# the service should be running

Exit the AWS instance
	instance terminal --> exit

We can browse the instance public IP from our local PC
	browser --> instance_public_ip

	# we should receive message set in the init.cfg - 'Deployed Machine via Terraform'

DESTROY
-------
We can now destroy the created resources on AWS
	terminal --> terraform destroy
	terminal --> yes			# confirm destruction








